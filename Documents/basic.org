#+TITLE: Basic Knowledge about LDA
#+AUTHOR: Quentin_Hsu
#+DATE: <2014-11-20 Thu>

原文转自：http://blog.csdn.net/pirage/article/details/8895083k
* VSM (向量空间模型)
计算机不具备人脑的结构，无法理解自然语言，所以需要首先将无结构到自然语言文本转化为计算机可计算的特征文本。向量空间模型(VSM)是20世纪60年代提出的一种文本表示模型，它将文档表示成特征元素(主要是文档中出现到词语)的集合，即D={t1,t2,...,tn}。最简单的计算词权重的方式是：如果词出现在文档中，则权值为1；没有出现，则权值为0。
这种方法的缺点在于，它没有体现词语在文档中出现的频率。一般来讲，词语在文档中出现的次数越多，说明它对该篇文档的重要性越大（此时，"是"、"和"、"的"等几十个停用词，应该作为例外被去除）。

* TF (词频)
TF：词出现的频率。但是，词语的重要性不仅随着它在文档中出现的次数成正比增加，而且也可能会随着它在语料库中出现的频率成反比下降。也就是说，一个词语在整体语料库中出现得越频繁，则它对于文档的重要性越低，对文档的区分度量越差。

* TF—IDF (词频-逆文档频)
它基于如下假设：对区别文档最有意义的词语应该是那些文档中出现频率高、而在整体语料库中的其他文档中出现频率少的词语。TF—IDF结构简单，容易理解，被广泛应用。但是，这种假设并不是完全正确，也无法捕捉文档内部与文档间的统计特征，更不能解决同义词/多义词问题，因此精确度不是很高。现在的搜索引擎对这种经典的文本降维技术进行了很多细微的优化（例如，考虑词出现在HTML结构的位置等），使其更加准确地衡量词语对文档的重要性。

* LSA (潜在语义分析)
鉴于TF—IDF存在一些缺点，Deerwester等人于1990年提出潜在语义分析（Latent Semantic Analysis）模型，用于挖掘文档与词语之间隐含的潜在语义分析。LSA的理论基础是数学中的奇异值矩阵分解（SVD）技术。

* PLSA (基于概率的潜在语义分析)
鉴于LSA存在一些缺点，Hofmann等人于1999年提出一种基于概率的潜在语义分析（Probabilistic Latent Semantic Analysis）模型。PLSA继承了“潜在语义”的概念，通过“统一的潜在语义空间”（也就是Blei等人于2003年正式提出Topic概念）来关联词与文档；通过引入概率统计的思想，避免了SVD的复杂计算。在PLSA中，各个因素（文档、潜在语义空间、词）之间的概率分布求解是最重要的，EM算法是最常用的方法。PLSA也存在一些缺点：概率模型不够完备；随着文档和词的个数增加，模型变得越来越庞大；在文档层面没有一个统计模型；EM算法需要反复迭代，计算量也大。

* LDA (潜在狄瑞雷克模型)
鉴于PLSA的缺点，Blei等人于2003年进一步提出新的主题模型LDA（Latent Dirichlet Allocation）,它是一个层次贝叶斯模型，把模型的参数也看作随机变量，从而可以引入控制参数的参数，实现彻底的“概率化”。
是LDA模型的Dirichlet的先验分布，表示真个文档集上主题的分布；表示文档d上主题的多项式分布；Z表示文档d的第n个词的主题；W表示文档d的第n个词；N表示文档d所包含词的个数；D表示文档集；K表示主题集；表示主题k上词语的多项式分布；表示所有主题上次的先验分布。事实上，去掉 和 ，LDA就变成了PLSA。目前，参数估计是LDA最重要的任务，主要有两种方法：Gibbs抽样法（计算量大，但相对简单和精确）和变分贝叶斯推断法（计算量小，精确度弱）。

* 其他基于topic model的演变
** 考虑上下文信息
例如：“上下文相关的概率潜在语义分析模型”（Contextual Probabilistic Latent Semantic Analysis, CPLSA）将词语上下文信息引入PLSA，也有研究人员考虑“地理信息”上下文信息，从地理位置相关的文档中发现地理位置关联的topic。
** 主题模型的演变
引入文本语料的时间信息，研究主题随时间的演变，例如DTM，CTDTM，DMM，OLDA等模型。
** 并行主题模型
在大规模数据处理的需求下，基于并行计算的主题模型也开始得到关注。现有的解决方案有：Mallet、GPU-LDA、Async-LDA、pLDA、Mahout、Mr.LDA等；其中pLDA、Mahout等基于Hadoop/MapReduce框架，其他方案则基于传统的并行编程模型；参数估计方面，Mallet、Async-LDA、pLDA等使用Gibbs抽样方法，Mr.LDA、Mahout等使用变分贝叶斯推断法，GPU-LDA同时支持两种方法。
