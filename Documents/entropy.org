#+TITLE: Information Entropy
#+AUTHOR: Quentin_Hsu
#+DATE: <2014-11-21 Fri>

信息熵解决了对信息的量化度量问题。
*** 定义
    对于任意一个随机变量X : H(X) = -∑P(x)logP(x)
    变量的不确定性越大，熵也就越大。
    [[/home/quentin/Downloads/test.jpg]]